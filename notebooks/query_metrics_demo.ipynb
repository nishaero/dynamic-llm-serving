{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Query FastAPI Prometheus Metrics\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates how to fetch and visualize metrics from your FastAPI app's Prometheus `/metrics` endpoint.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import requests\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"# FASTAPI_METRICS_URL can be localhost if running in the same container, else use the public endpoint\\n\",\n",
    "    \"FASTAPI_METRICS_URL = \\\"http://localhost:8080/metrics\\\"\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Fetch metrics\\n\",\n",
    "    \"resp = requests.get(FASTAPI_METRICS_URL)\\n\",\n",
    "    \"metrics_text = resp.text\\n\",\n",
    "    \"print(metrics_text[:500])  # Print first 500 chars for inspection\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Parse Inference Latency Histogram Buckets\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Example Prometheus histogram line:\\n\",\n",
    "    \"# inference_latency_seconds_bucket{le=\\\"0.005\\\"} 0.0\\n\",\n",
    "    \"\\n\",\n",
    "    \"latency_buckets = {}\\n\",\n",
    "    \"for line in metrics_text.splitlines():\\n\",\n",
    "    \"    if line.startswith('inference_latency_seconds_bucket'):\\n\",\n",
    "    \"        m = re.match(r'.*le=\\\"([0-9e\\\\+\\\\.-]+)\\\"} ([0-9\\\\.]+)', line)\\n\",\n",
    "    \"        if m:\\n\",\n",
    "    \"            le = float(m.group(1))\\n\",\n",
    "    \"            count = float(m.group(2))\\n\",\n",
    "    \"            latency_buckets[le] = count\\n\",\n",
    "    \"\\n\",\n",
    "    \"if not latency_buckets:\\n\",\n",
    "    \"    print(\\\"No latency buckets found. Have you made any /generate requests?\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(latency_buckets)\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Plot Latency Histogram\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"if latency_buckets:\\n\",\n",
    "    \"    plt.figure(figsize=(8,4))\\n\",\n",
    "    \"    buckets = sorted(latency_buckets.items())\\n\",\n",
    "    \"    x = [le for le, _ in buckets]\\n\",\n",
    "    \"    y = [count for _, count in buckets]\\n\",\n",
    "    \"    plt.step(x, y, where='post')\\n\",\n",
    "    \"    plt.xlabel('Latency (seconds)')\\n\",\n",
    "    \"    plt.ylabel('Cumulative Requests')\\n\",\n",
    "    \"    plt.title('Inference Latency Histogram')\\n\",\n",
    "    \"    plt.grid(True)\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Plot Number of Model Switches\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Extract MODEL_SWITCHES counter\\n\",\n",
    "    \"model_switches = None\\n\",\n",
    "    \"for line in metrics_text.splitlines():\\n\",\n",
    "    \"    if line.startswith('model_switches_total'):\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            model_switches = float(line.split()[-1])\\n\",\n",
    "    \"        except Exception:\\n\",\n",
    "    \"            pass\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Model switches: {model_switches}\\\")\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
